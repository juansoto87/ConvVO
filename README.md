# ConvVO
 Convolutional Visual Odometry Model
### General Considerations ##

This paper presents an unsupervised model for the determination of the odometry of a robot using monocular image sequences. For this purpose, a convolutional network was developed that processes pairs of consecutive images, providing the relative translation and rotation between the two images.
Using the position vector provided by the model and the depth map of the It image obtained using the 
depth map of the It image obtained using a pre-trained model, the I't+1 image was synthesized and by comparing this generated image with the original It+1 image, the error was calculated and used during the training phase of the was used during the model training phase.

![image](https://github.com/juansoto87/ConvVO/assets/70484982/03024062-4343-4baf-973d-972cbe76b590)

In the diagram, the block diagram of the model proposed in the present master project is observed. A pre-trained model is used to determine the depth maps of the scenes [T. Shen et al, "Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation," Feb. 2019, [Online]. Available: http://arxiv.org/abs/1902.09103] this model uses a convolutional autoencoder structure whose output is the depth map of a monocular scene. Finally, a convolutional network is trained for the determination of the position change vector using pairs of consecutive It and It+1 images.
   
Subsequently, using the depth map obtained from the pre-trained model for the It image, the spatial coordinates (X, Y, Z) are retrieved for each of the image pixels that lie in the l-plane.
 
From the position change vector, the rotation and translation matrix is estimated to transform the coordinates of the It image points so as to obtain a synthesized image of the It+1 frame. In other words, this provides a view of the initial image from the new camera position represented by the position vector.

 ![image](https://github.com/juansoto87/ConvVO/assets/70484982/a9502bbf-752f-4bbb-8fdb-5727f525a979)

Equation shows the transformation from point x0 to point x1 shown in Figure 9, where K corresponds to the intrinsic matrix of the camera and M10 is the transformation matrix formed by the rotation and translation matrix described in the equation.

![image](https://github.com/juansoto87/ConvVO/assets/70484982/9ec06f25-86b1-428a-acbb-068d693ceff1)


It is also observed in the diagram, the position estimation block consisting of a deep network, composed of four convolutional blocks with residual connections, five simple convolutional layers and 6 dense layers. The filter size of the residual blocks are k = (7,5,3,3) with map size reduction via stride instead of pooling, in order to preserve the location of the detected features [24] reaching up to a size of 8x24 and increasing the number of maps to obtain 512.
Subsequently, we move to four convolutions with filter size k=1 reducing the number of maps up to 32 and keeping the dimensions in 8x24, in this way we achieve a dimensional reduction of the maps preserving the spatial information related to the features detected in the previous residual blocks [25]. 
Finally, the obtained maps are flattened and connected with four dense layers delivering a vector of size 32 which is connected to two dense layers of size 3 representing the translation [x, y, z] and rotation [rx, ry, rz] vector respectively.
For both the convolutional networks and the dense layers, the linear rectified unit ReLU was used as the activation function except for the two output layers, for which a linear activation function was used, because it was required that they could take negative values.

As a last consideration of the model, it was decided to cut the border of the images to calculate the error, because when there are strong rotations between frames, new elements appear in the It+1 image that cannot be inferred by means of the transformation from the It frame. 

![image](https://github.com/juansoto87/ConvVO/assets/70484982/0f0d2eac-7d6e-4577-b266-554971fbbb3f)


The phenomenon is observed in the next figure where new pixels on the right edge of the It+1 image cannot be generated in the synthesized I't+1 image, resulting in an increase in the error calculated during training, which may penalize correct predictions of the network.

![image](https://github.com/juansoto87/ConvVO/assets/70484982/1e244a4a-d1c8-4428-9c51-cf1106467dfb)


Figure bellow shows the effect achieved by cropping the edges of the image, eliminating the empty pixels that are generated by applying a rotation on the image at t and comparing the position of the pixels contained in both It and It+1 in their cropped versions.

![image](https://github.com/juansoto87/ConvVO/assets/70484982/6f37d8a6-514a-4345-a015-8c1d9f851221)

### Dataset used ###

The KITTI dataset was used [A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, "Vision meets Robotics: The KITTI Dataset." [Online]. Available: http://www.cvlibs.net/datasets/kitti.] consisting of video data collected by a set of stereo cameras, one color and one grayscale, plus validation data from a laser scanner for scene depth determination and a GPS sensor from which odometry can be obtained. All these sensors were installed in a vehicle, which made urban and rural routes. The dataset is grouped in 10 sequences.
It was decided to eliminate sequences 01 and 04 from the project database since they differ from the predominant type of environment of the sequences, which mostly corresponds to residential routes, and on the other hand, they present a large number of dynamic objects, which generates problems when inferring the displacement as the model has been proposed.

![image](https://github.com/juansoto87/ConvVO/assets/70484982/57ac1c4a-80fa-49da-97ac-45d0d01f0900)

** Use  preprocess.py to prepare data** 
### Training and Results ###

Check Jupyter notebook J_VO.ipynb.

After 500 training epochs, best model was found around epoch 231.

![image](https://github.com/juansoto87/ConvVO/assets/70484982/b2a88cec-baf4-4db0-ba9c-d2c68913058c)


The figure below shows how the model "Learn from data" by generating better I't+1 (c) view based on It (a) and depth map(b), subfigure d correspond to It+1 image.

![image](https://github.com/juansoto87/ConvVO/assets/70484982/e5c0c2f4-1df9-417e-81b4-82912b307050)

After training sequences 09 and 10 were pass trhough the model

![image](https://github.com/juansoto87/ConvVO/assets/70484982/304e8742-3022-44f3-85a0-ff4b5e1bbfae)

Model avaible in https://drive.google.com/file/d/1NrPe7-8Q1djvM33x0I8Lf2zydqJjPOuS/view?usp=sharing
